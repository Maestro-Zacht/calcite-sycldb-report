\documentclass[a4paper, 10pt]{article}
\usepackage[top=3cm, bottom=3cm, left = 2cm, right = 2cm]{geometry}
\geometry{a4paper}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage[parfill]{parskip}
\usepackage{pgfplotstable,filecontents}
\usepackage{booktabs}

\pgfplotsset{compat=1.18}

\graphicspath{{./images/}}

\hypersetup{
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=black
}

\title{Calcite for SYCLDB report}
\author{Matteo GHIA}

\begin{document}

\maketitle

\section{Introduction}

In this report I'm going to describe the work done with Apache Calcite, Apache Thrift and SYCLDB in details.

The system is composed of three main components:
\begin{itemize}
    \item \textbf{Apache Calcite}, a SQL parser, validator, optimizer and planner.
    \item \textbf{Apache Thrift}, a RPC framework for scalable cross-language services development.
    \item \textbf{SYCLDB}, a database system that uses SYCL for parallel query execution.
\end{itemize}
Apache Calcite and SYCLDB run in their own processes, while Apache Thrift is used to communicate between them.

In section \ref{performances}, I'll present performance measures over Apache Calcite query processing and Apache Thrift communication.

In section \ref{setup}, I'll show how to get the system running on a new machine.

In section \ref{future_work}, I'll list some possible improvements that can be applied to the project.

\section{Apache Calcite}

Apache Calcite has been used to parse, validate and optimize SQL queries in order to get a physical plan that can be executed by SYCLDB. It is written in Java and provides a rich set of features for SQL processing. The project is called \texttt{sycldb-adapter} because at the beginning it was meant to be a Calcite adapter for SYCLDB and thus calling SYCLDB functions directly, but during the development I've chosen to separate the two components.

There are two main parts in this project:
\begin{itemize}
    \item The first part is \textbf{src/main/java/com/eurecom/calcite}, which contains the classes needed by Calcite.
    \item The second part is \textbf{src/main/java/com/eurecom/calcite/thrift}, which contains the classes needed to integrate Calcite with Apache Thrift.
\end{itemize}

\subsection{Calcite classes}

Most of the files in the first part follow the pattern \texttt{<OperationName>} and \texttt{<OperationName>Rule}, where the first file contains the implementation of an operation or an object, and the second file contains the rule that has to be added to the query planner in order to make it use that operation or object.
\\
All the \texttt{<OperationName>Rule} files contain just boilerplate code used to convert from a logical operator to the corresponding physical operator which they are describing. The only important thing to notice here is that all the rules are registered using the SYCLDB convention. Conventions in Calcite are used to group together operators that have something in common, such as the same execution engine in this case.
\\
The operation/object files define the implementation of the operation or object by extending the corresponding Calcite class or interface. The constructor of these classes is used to set the properties of the operation or object required by the superclass and optionally additional properties specific to the class. They also implement the \texttt{copy} method, which is required by Calcite and it is just boilerplate code. There are also other two methods: \texttt{implement} and \texttt{computeSelfCost}. The first one was used in the code generation step when this project was expected to be a Calcite adapter, but it is not used anymore and empty in most classes. The second one is used to compute the cost of the operation and it used to be a simple reduction by a factor of ten, in order to make the optimizer prefer SYCLDB operations over default Calcite operations. This is not useful anymore since now we use only SYCLDB operations, and the code should be updated after adding metadata (more on this later) and should take into account number of rows and columns.

There are other files in the first part that are not following this pattern and they have various purposes:
\begin{itemize}
    \item \textbf{SycldbSchema} is the class that represents the database schema of SYCLDB. At the moment it is just the hardcoded SSB schema.
    \item \textbf{SycldbTable} is the class that represents a table in SYCLDB. It is used in the \texttt{SycldbSchema} class to create the tables of the schema. It extends the class \texttt{AbstractQueryableTable} which makes Calcite recognize it as a table on which Relational operations can be applied, as opposed on a table which can only be read and no operations can be applied on it, for example like a Redis index. This is done by implementing the \texttt{asQueryable} method which returns a generic class, the SycldbQueryable subclass that contains just boilerplate code. The table also implements the \texttt{ProjectableFilterableTable} interface, which provides a \texttt{scan} method that should be used to apply filters and projections on the table directly without intermediate expressions. I was not able to make this work, so I left it as a placeholder.
    \item \textbf{ProjectTableScanRule} was a second attempt to make the optimizer push down projections on the table scan, but it was not successful. The rule is not added to the planner anymore, and I left it for future reference.
    \item \textbf{SycldbRel} is the class that represents a SYCLDB relational operation. It contains some subclasses that were used when this project was expected to be a Calcite adapter, but they are not used anymore. The class is just used as a base class for the other SYCLDB operations, since it extends \texttt{RelNode} and all the opration in Calcite have to extend that class. The only useful information left in this class is the SYCLDB convention definition.
    \item \textbf{SYCLDBQueryProcessor} was the initial entry point for the Calcite adapter, but it is not used anymore.
    \item \textbf{SycldbEnumerable} is the class that represents a SYCLDB result for the Enumerable convention. It was used to convert data from SYCLDB convention to the Enumerable convention, but it is not used anymore since now we use only SYCLDB operations.
    \item \textbf{SycldbToEnumerableConverter} and \textbf{SycldbToEnumerableConverterRule} actually follow the pattern described previously, but they are not used anymore since now we use only SYCLDB operations. They were used to define the operation to convert from SYCLDB to Enumerable convention and the corresponding rule to add to the planner.
    \item \textbf{SycldbJsonConverter} is the class that converts a JSON object given by Calcite default JSON dump plan into Apache Thrift objects (more on this later).
\end{itemize}

\subsection{Thrift integration}

The second part of the project is used to integrate Calcite with Apache Thrift. It contains the autogenerated classes from the Thrift file \texttt{thrift/calciteserver.thrift}. It also contains the classes needed to implement the Thrift server, which are \texttt{ServerCaller}, \texttt{ServerWrapper} and \texttt{ServerHandler}. The first two contain just boilerplate code to start and run the server, while the third one contains the code for handling the requests. The requests for the \texttt{ping} and \texttt{shutdown} methods just print a message and return, while the \texttt{parse} method is the one that handles the SQL query. It takes the SQL string as input and performs all the steps required to get the physical plan that will be passed to SYCLDB for execution.
\begin{itemize}
    \item \textbf{Query parsing and definition of the schema}. The SQL string is converted into an AST which is implemented by the Calcite class \texttt{SqlNode}.
    \item \textbf{Validation of the query and convert to logical plan}. The schema is used to define a \texttt{catalogReader} which contains the information about the tables and their columns. At the moment it is just a collection of tables and columns, with no metadata. The parsed AST is then validated and we obtain a \texttt{SqlNode} which contains the validated query tree. This data structure is then converted into a logical plan, which is represented in the class \texttt{RelNode}.
    \item \textbf{Optimization into physical plan}. The logical plan is then converted into a physical plan via the planner. Conversion rules and optimization rules are added in order to make the optimizer recognize Sycldb operations, which have been defined in the classes described previously, and basic built-in optimizations such as selection push-down.
    \item \textbf{Conversion into Thrift classes}. Using the \texttt{SycldbJsonConverter} described previously, the JSON representation of the physical plan is used to build the Thrift classes, which are then returned by the function and serialized and sent by the Thrift server to the client which made the request.
\end{itemize}

\section{Apache Thrift}

Apache Thrift has been used as a bridge between Apache Calcite and SYCLDB executor. It takes care of running the HTTP server and providing a RPC interface which abstracts all the physical communication.

The file which contains the interfaces and data structures definitions is placed inside the \texttt{sycldb-adapter} project at the path \texttt{thrift/calciteserver.thrift}. The interfaces are defined in the \texttt{service} block, while the data structures are in \texttt{struct}/\texttt{enum} blocks. Interfaces are just functions and don't require to be explained, while data structures require more attention.
\begin{itemize}
    \item \textbf{PlanResult} is the result returned by the \texttt{parse} function. It contains two fields:
          \begin{itemize}
              \item \textbf{oldJson} is the original JSON which is given by Calcite. It is returned just for debug and development purposes.
              \item \textbf{rels} is the list of operations after they have been converted into Thrift data structures by the \texttt{SycldbJsonConverter} class in Calcite. It is a list of \texttt{RelNode} objects (not the Calcite class, but the Thrift struct described later).
          \end{itemize}
    \item \textbf{RelNode} is the top-level class describing an operation. It contains various fields which depend on the type of operations, except \texttt{id} which is the progressive id of the operation, and \texttt{relOp} which is the enum that identifies the type of operation. The other fields are specific to a certain type of operation and they are \texttt{null} in other operation types.
          \begin{itemize}
              \item \textbf{tables} is a list of string containing the table names in a table scan. It is always of length 1, but I've left it as a list since it a list in the original JSON.
              \item \textbf{inputs} is present in table scan and join operations. It is always empty in SSB table scans, but it may have a use in general. In joins it represents the operation ids whose output is used in the join operation.
              \item \textbf{condition} is present in joins and selections. It represent the filtering condition in both cases and it can be a recursive data structure (described later).
              \item \textbf{joinType} is the type of the join. It is always equal to "inner" in my implementation, but it could be a different type if the correct rules and configurations are added to Calcite.
              \item \textbf{fields} are the output field names of a project operation.
              \item \textbf{exprs} is the list of expressions describing the output fields of a project operation.
              \item \textbf{group} are the column indexes on which a group by is applied in an aggregate operation.
              \item \textbf{aggs} is the list of aggregates to apply in an aggregate operation. It always contains only one value for SSB.
          \end{itemize}
    \item \textbf{ExprType} describes an expression and can be a nested data structure. It can be of three types, described in the \texttt{exprType} enum field: literal, column or expression.
          \begin{itemize}
              \item \textbf{literal}: the instance represents a literal number or a range of values. The \texttt{type} string attribute represents which type of literal is, while the \texttt{literal} field contains the data about the literal.
              \item \textbf{column}: the instance represent a column reference. The \texttt{input} field represents the column index and the \texttt{name} field represents the column name.
              \item \textbf{expression}: the instance represents a nested expression. The \texttt{op} string attribute represents the operation between the input operands and the \texttt{operands} field is the list of operands. Sometimes \texttt{type} field is present in the original JSON representation and it is kept here, but it is currently not used in the executor code.
          \end{itemize}
    \item \textbf{LiteralType} contains the information about a literal. The \texttt{literalOption} encodes if the information is about a single literal or a range. In the first case, \texttt{value} contains the literal. In the second case, \texttt{rangeSet} contains the range. There are two possibilities for a range:
          \begin{itemize}
              \item \textbf{set} is a set of values between a minimum and a maximum. The attribute is always a list of size one, with the sub list representing the range. The first value of the list represent which kind of set is and it is always a "closed" set in our case, i.e. minimum and maximum are included. The following two values in the list represent minimum and maximum.
              \item \textbf{singletons} encodes different values that are possible, i.e. a list of equal conditions in logical OR. The attribute is a list of variable length and each element is a list that contains the information about one value. in the sub list, the first element is always the string "singleton", while the second represents the actual number.
          \end{itemize}
    \item \textbf{AggType} contains the information about one aggregation.
          \begin{itemize}
              \item \textbf{agg} is a string representing the aggregation to be performed (e.g. SUM)
              \item \textbf{operands} contains the column index on which the aggregation is performed. It is a list as in the original JSON representation, but it only contains one value for SSB.
              \item \textbf{name} is the name of the output of the aggregation.
              \item \textbf{type} is the type of the output. Currently it is not used in the executor.
              \item \textbf{distinct} marks if the aggregation contains a \texttt{DISTINCT} keyword. It is not used in SSB.
          \end{itemize}
\end{itemize}

\section{SYCLDB executor}

The SYCLDB executor is the component that actually executes the query. It is written in SYCL, an abstraction on top of C++ for parallel execution over multiple heterogeneous platforms. The project is called \texttt{sycldb-calcite-executor} and I've worked on code and algorithms which run in plain C++, while Ivan KABADZHOV has implemented the operations in SYCL.

The job of the executor is reading the SQL file containing the query, sending it over Apache Calcite via Apache Thrift, reading the physical plan obtained and executing the operations. I'll focus on the last part since the first ones are trivial.

The physical plan result obtained from Calcite in the \texttt{main} function is passed to the \texttt{execute\_result} function and then read three times. The first time happens in the \texttt{parse\_execution\_info} function and it is used for detecting which columns are used in order to load just those ones, which is the last operation id where a table is used in order to detect semi joins, and the list of columns on which a group by is applied. The second time is for performing table scans and load the data all at once before performing any other operation. The third time is for actual execution.

There are some support variables used in the \texttt{execute\_result} function in order to keep track of the various data.
\begin{itemize}
    \item \textbf{tables} contains the actual table data, and it is a static array of length 5.
    \item \textbf{current\_table} is used to keep track of the first free index in the \texttt{tables[]} array.
    \item \textbf{output\_table} is a dynamic array and it is used to keep track of the index in \texttt{tables[]} which is the output of a certain operation id (i.e. \texttt{output\_table[5]} gives the output table index of operation 5).
    \item \textbf{exec\_info} holds the data obtained by the first run on the physical plan. In particular, this data is:
          \begin{itemize}
              \item \textbf{loaded\_columns} which is a map that maps table names into a set of column indexes, and represents which columns have to be loaded for a specific table.
              \item \textbf{table\_last\_used} is a map that maps table names into the last id in which they are used.
              \item \textbf{group\_by\_columns} is a map that maps table names into the column used in the group by operation. In SSB, this is just a single column per table.
          \end{itemize}
\end{itemize}

Columns and tables are stored along with metadata in \texttt{ColumnData} and \texttt{TableData} structs respectively. In the first one, these are the fields:
\begin{itemize}
    \item \textbf{content} is the actual pointer that will contain the data.
    \item \textbf{has\_ownership} marks if the pointer is the owner of the data or is just a reference to something stored somewhere else. While this should be enforced, we prioritized implementing more features over memory correctness: there is a bug somewhere in the code that causes some pointer to be freed twice, so we disabled all memory free.
    \item \textbf{is\_aggregate\_result} marks if the current columns is the result of an aggregation. If this is the case, the \texttt{content} pointer contains \texttt{uint64\_t} instead of \texttt{int} values.
    \item \textbf{min\_value} and \textbf{max\_value} should keep track of min and max of the column but there is a bug somewhere that makes them incorrect, so they are recomputed in the aggregate function.
\end{itemize}

In \texttt{TableData}, these are the fields:
\begin{itemize}
    \item \textbf{columns} is the array of \texttt{ColumnData}.
    \item \textbf{columns\_size} is the length of the \texttt{columns} array. Note that it can be different than the number of columns in the table.
    \item \textbf{col\_len} is the number of rows in the table.
    \item \textbf{col\_number} is the number of columns in the table.
    \item \textbf{flags} is the boolean array representing the selection flag for every row.
    \item \textbf{group\_by\_column} is the column used in the group by operation ($-1$ if no column is grouped on).
    \item \textbf{table\_name} is self explanatory.
    \item \textbf{column\_indices} is a map that maps column index from Calcite into the index in the \texttt{columns} array. This is used when there are less columns loaded and the indexes do not match.
\end{itemize}

After having loaded all the tables, the actual execution step begins. Every operation is performed in its own function, which I'll describe below.
\begin{itemize}
    \item \textbf{parse\_filter}. Since expressions are a recursive data structure, also this function is recursive. It receives the expression, the table data and the parent logical operation and it applies the action described in the expression. There are multiple possible actions that can happen:
          \begin{itemize}
              \item \textbf{rangeSet}: the filter is a rangeSet condition, which is either a range or a list of possible values. This is handled in the branch where the \texttt{op} is "SEARCH"
              \item \textbf{logical}: the filter is a logical operation between other expressions. The function recurses on these expressions. For SSB, passing the parent operation to the first recursion and applying the operation of the filter in the other calls is correct, but may not work in general.
              \item \textbf{comparison}: the filter is a comparison between two operands. After getting the pointer to the two columns or the column and the literal, the selection kernel is applied.
          \end{itemize}
    \item \textbf{parse\_project}. Projection redefines the columns, so it requires us to create a new \texttt{columns} array. This is then filled according to the info contained in the array of expressions. In particular there are three cases:
          \begin{itemize}
              \item \textbf{column}: the pointer to the content array is moved over and the metadata is updated.
              \item \textbf{literal}: the column is filled with the literal value. This never happens in SSB.
              \item \textbf{expression}: Calcite puts the operations inside aggregation in a project operation first, then applies the aggregation at a later step. This is reflected into the expression case of the project. The two operands are extracted (two columns or column and literal) and the appropriate \texttt{perform\_operation} function is called.
          \end{itemize}
    \item \textbf{parse\_join}. There are two possible joins: semi join and full join.
          \begin{itemize}
              \item \textbf{semi join} happens if the right table is last used at the current operation id, and the join becomes just a filter for the flags of the left table. An hash table is built on the right table and the flags of the left table are updated if the foreign key hash is present or not in the hash table.
              \item \textbf{full join} instead is more complex. The hash table is built with two cells for each hash: the presence of the hash (integer equal to 0 or 1) and the value of the group by column. This is done since we can avoid merging the columns of the right table into the left table since the only used column after a join in SSB is the group by column, and we replace the foreign keys in the left table with the group by column value. This can be done in O(1) by adding this value in the second cell corresponding to the hash in the hash table and doing the replacement during the probe phase.
          \end{itemize}
          After the join is applied, in both cases the \texttt{col\_number} attribute of the left table is increased, since for Calcite we have merged the columns and the indexes in the following operations will reflect this.
    \item \textbf{parse\_aggregate}. There are two possibilities of aggregate operation: with or without group by.
          \begin{itemize}
              \item \textbf{without group by} is just an aggregation that will result in a single value. This is an \texttt{unsigned long long} and it is stored in a column of length 1 which has the \texttt{is\_aggregate\_result} flag set to true.
              \item \textbf{with group by} is more complex. The function passes all the needed pointers and operation data to the \texttt{group\_by\_aggregate} function where everything exceptupdateing the data structure happens. Here we build some support variables: \texttt{max\_values} and \texttt{min\_values} hold each column max and min, and \texttt{prod\_ranges} holds \[ \prod^{col\_num}_i \mathrm{max}[i] - \mathrm{min}[i] + 1 \] which is used to compute the hash of multiple values. The hash set is a fixed oversized array which will contain all the hashes of already inserted tuples and the result array holds all the group by column plus the aggregation column, which is of type \texttt{uint64\_t}. Then the hash table is built along with the hash set and result arrays containing the group by columns. The aggregation is done at this stage, since the hash table holds the aggregate result for every hash. At the end, results from the hash table are inserted in the result arrays, in the calling function metadata and pointers are updated.
          \end{itemize}
\end{itemize}

\section{Performance measures} \label{performances}

In this section I'm going to provide some performance analysis over the overhead of Apache Calcite + Apache Thrift, which impacts the query execution time.

Measures are taken for every query by restarting the Calcite server, in order to have a clean state, and running the same query 1100 times. The first 100 runs are discarded, while the remaining 1000 runs are used to compute the average and standard deviation of the execution time. This is done in order to avoid measuring the warmup phase of the JVM.

The code for the measures is placed in the \texttt{sycldb-calcite-executor} project in the \texttt{performances} branch. Results are in the \texttt{performances} folder. Files named \texttt{<qxx>.txt} contain the times on C++ side, while files named \texttt{<qxx>-calcite.txt} contain the times on Calcite side. The Jupyter notebook \texttt{performances.ipynb} contains the code to plot the results and compute the average and standard deviation, which has been stored in the file \texttt{performance\_results.csv}. Thrift times are computed by subtracting the time taken by Calcite to process the query to the time measured on C++ side.

Here is a table summarising the results. All the numbers are in microseconds.

\pgfplotstabletypeset[col sep=comma,
columns={Query,C++ Mean,C++ Std,Java Mean,Java Std,Thrift Mean,Thrift Std},
columns/Query/.style={string type},
columns/C++ Mean/.style={fixed,precision=0},
columns/C++ Std/.style={fixed,precision=0},
columns/Java Mean/.style={fixed,precision=0},
columns/Java Std/.style={fixed,precision=0},
columns/Thrift Mean/.style={fixed,precision=0},
columns/Thrift Std/.style={fixed,precision=0},
every head row/.style={before row=\toprule,after row=\midrule},
every last row/.style={after row=\bottomrule}
]{performance_results.csv}

\section{Setting up the system} \label{setup}

\subsection{Apache Calcite}

This is the easiest part of the installation. A JVM is all that is needed to run Calcite. The project has been developed with \texttt{openjdk-21-jdk} Debian package.

\subsection{Apache Thrift}

Installing Apache Thrift instead is more complex, since it needs to be compiled from source code.

Install general dependencies:
\begin{verbatim}
sudo apt-get update
sudo apt-get install -y \
    automake bison flex g++ git libboost-all-dev \
    libevent-dev libssl-dev libtool make pkg-config
\end{verbatim}

Install Java dependencies:
\begin{verbatim}
sudo apt-get update
sudo apt-get install -y \
    ant ant-optional maven openjdk-17-jdk-headless
export GRADLE_VERSION="8.4"
wget https://services.gradle.org/distributions/gradle-$GRADLE_VERSION-bin.zip -q -O \
    /tmp/gradle-$GRADLE_VERSION-bin.zip \
    (echo "3e1af3ae886920c3ac87f7a91f816c0c7c436f276a6eefdb3da152100fef72ae  \
    /tmp/gradle-$GRADLE_VERSION-bin.zip" | sha256sum -c -)
unzip -d /tmp /tmp/gradle-$GRADLE_VERSION-bin.zip
sudo mv /tmp/gradle-$GRADLE_VERSION /usr/local/gradle
sudo ln -s /usr/local/gradle/bin/gradle /usr/local/bin
sudo update-java-alternatives --set /usr/lib/jvm/java-1.17.0-openjdk-amd64
\end{verbatim}

Install C++ dependencies:
\begin{verbatim}
    sudo apt-get install -y \
    libboost-all-dev libevent-dev libssl-dev qtbase5-dev qtbase5-dev-tools
\end{verbatim}

Install Thrift:
\begin{verbatim}
wget https://archive.apache.org/dist/thrift/0.21.0/thrift-0.21.0.tar.gz
tar -xvf thrift-0.21.0.tar.gz
cd thrift-0.21.0
./bootstrap.sh
./configure --without-c_glib --without-python --with-java \
    --without-py3 --without-kotlin --disable-tutorial --disable-tests
sudo make -j4
sudo make install
\end{verbatim}

In order to test if Thrift is installed:
\begin{verbatim}
thrift -version
\end{verbatim}

In order to compile the generated code for java and c++:
\begin{verbatim}
thrift -r --gen java calciteserver.thrift
thrift -r --gen cpp calciteserver.thrift
\end{verbatim}
in the folder containing the thrift file. This will generate two folders, \texttt{gen-java} and \texttt{gen-cpp}. The content of the first one should be put into \texttt{src/main/java/com/eurecom/calcite/thrift} of the Calcite project, while the second folder needs to be put at the top level of the SYCLDB executor project.

\subsection{Running the system}
In order to run the system, first start the Calcite server:
\begin{verbatim}
gradle run
\end{verbatim}
in the Calcite project folder.

Then, in a different terminal, run the SYCLDB executor:
\begin{verbatim}
make
./client <query_file_path>
\end{verbatim}

\section{Future work} \label{future_work}

There are many possible improvements that can be applied to the project, most of them are related to the Calcite part of the project.
\begin{itemize}
    \item \textbf{Metadata}. At the moment, the Calcite schema is just a collection of tables and columns, with no metadata. This can be improved by adding metadata which can in turn be used in the cost function and in general optimizations. For example, the number of rows and columns can be used to compute the cost of an operation, for example suggesting the optimizer to add a projection right after a table scan in order to reduce the number of columns and thus the amount of data to be processed in the following operations.
    \item \textbf{Improved operations}. There are some operations that can be improved, such as the join operations. At the moment, only the inner join is taken into account by the optimizer, but it can be extended to support other join types such as semi joins.
    \item \textbf{Custom dump plan}. The JSON dump plan provided by Calcite has a lot of information that is not used in the executor or it can be improved with extra information. Writing a custom dump plan that contains only the information needed by the executor will also reduce the computation done on Java side, since currently the JSON dump plan is parsed and then converted into Thrift classes, which is an extra step that can be avoided.
    \item \textbf{Reordering of operations}. The optimizer can be improved by adding rules that reorder operations in order to support kernel fusion in SYCL. This can be done as an initial solution by reordering manually by using a topological sort of the DAG of operations, but it can be improved by adding optimization rules that reorder operations directly in the planner.
\end{itemize}
\end{document}